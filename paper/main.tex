\documentclass{article}
\usepackage[UTF8]{ctex}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{开放目标检测技术研究综述}
\author{xxx}
\date{\today}

\begin{document}

\maketitle

\section{引言与背景 (Introduction \& Background)}
\subsection{研究背景与动机}
\subsubsection{传统目标检测的成功与局限}
在过去十年中，深度学习驱动的目标检测技术取得了巨大突破。从区域提议网络（R-CNN系列）\cite{ren2017faster} 到单阶段检测器（YOLO系列、SSD、RetinaNet），再到基于 Transformer 的端到端检测器（DETR系列），目标检测在精度和速度上都实现了质的飞跃。这些方法在 COCO、PASCAL VOC 等标准数据集上屡创新高，推动了自动驾驶、医疗影像分析、安防监控等领域的实际应用。

然而，传统目标检测方法严格受限于\textbf{封闭世界假设（CWA）}。在这一假设下，模型的分类体系由训练数据集预先定义且固定不变。例如，在 COCO 数据集上训练的检测器只能识别其定义的 80 个类别，对于任何不在训练集中的物体，模型要么将其错误分类为相似的已知类别，要么直接忽略为背景。

封闭世界假设的核心问题主要体现在两个方面。\textbf{静态类别空间}：一旦模型训练完成，其可识别的类别集合就被固定。当需要检测新类别时，必须重新收集数据、标注并重新训练整个模型。\textbf{排他性判定机制}：传统检测器的分类头学习的是从图像特征到离散类别 ID（如 0, 1, ..., 79）的映射关系，这些 ID 本身不包含任何语义信息。模型通过 Softmax 函数强制每个候选区域归属于 $N$ 个已知类别之一，本质上是一种"封闭集合多分类"问题：
\[
P(c_i|x) = \frac{e^{z_i}}{\sum_{j=1}^{N} e^{z_j}}, \quad c_i \in \{c_1, c_2, ..., c_N\}
\]
其中 $x$ 是区域特征，$z_i$ 是对应类别的 logit。这种机制天然地排斥了 $N+1$ 类的存在。

\subsubsection{现实世界的长尾分布与标注瓶颈}
封闭世界假设与现实世界的开放性、动态性存在根本性矛盾：

\textbf{（1）极端的长尾分布}

现实世界中的物体类别分布服从极端的长尾定律（Zipf's Law）。以 LVIS 数据集\cite{gupta2019lvis}为例，其包含 1203 个类别，但这些类别的出现频率极不均衡：\textbf{频繁类}（如 "person"、"car"）在数据集中出现数千次，\textbf{常见类}（如 "guitar"、"laptop"）出现数百次，而\textbf{罕见类}（如 "accordion"、"trombone"）仅出现数十次。

更重要的是，即便是包含 1203 类的 LVIS，相对于真实世界中数以百万计的物体类别（考虑不同品牌、型号、状态的细分），依然是沧海一粟。传统方法试图通过不断扩充数据集来覆盖更多类别，但这种策略在数学上是不可持续的。

\textbf{（2）标注成本的指数级增长}

假设标注一张图像中所有物体的平均成本为 $C$，类别数量为 $N$，那么覆盖 $N$ 个类别所需的标注成本为：
\[
Cost_{total} = C \times N \times k
\]
其中 $k$ 是每个类别所需的样本数（通常需要数千张以保证训练效果）。当 $N$ 从 80（COCO）增长到 1203（LVIS）再到 10000+（开放世界）时，所需的人工标注成本呈指数级增长，这在经济上和时间上都是不可接受的。

更关键的问题在于，即使投入巨大资源标注了大量类别，模型部署后仍会不断遇到训练时未见过的新物体（如新发布的产品型号、罕见的动植物种类、特定场景下的临时物体等），导致模型性能快速衰减。

\subsubsection{实际应用场景的迫切需求}
在真实的应用场景中，封闭世界假设带来的局限性尤为明显。在\textbf{自动驾驶}场景中，道路上可能出现工程车辆、临时路障、罕见动物等训练集中不存在的物体，模型必须能够识别这些"未知"目标以保证安全。在\textbf{智能安防}领域，监控系统需要检测异常物体（如可疑包裹、非法侵入的动物），这些物体往往无法预先定义。在\textbf{医疗影像分析}中，罕见疾病的影像特征可能在训练集中缺失，但临床诊断必须能够识别并标记这些异常区域。

这些场景共同指向一个核心需求：\textbf{模型必须具备在开放、动态环境中持续学习和适应的能力}，而不是被固定的训练数据所束缚。

下图直观对比了 (a) 传统封闭集检测与 (b) 开放集检测中人类输入 novel categories 的能力：
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../picture/p3.png}
    \caption{封闭集检测与开放集检测的对比}
    \label{fig:p3}
\end{figure}

因此，打破 CWA，构建能够适应动态环境、低成本扩展类别的检测系统，成为计算机视觉领域的必然趋势。

\subsection{范式转移：视觉-语言融合的突破}
\subsubsection{从离散 ID 到语义嵌入}
打破封闭世界假设的关键在于\textbf{改变类别表示的方式}。传统检测器将类别表示为离散的数字 ID，这种表示缺乏语义内涵，无法泛化到新类别。而视觉-语言模型（VLM）的出现为这一问题提供了革命性的解决方案。

\textbf{传统分类器 vs. 开放检测器的本质区别：}
\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}lcc@{}}
        \toprule
        对比维度   & 传统分类器                                     & 开放检测器                                \\
        \midrule
        类别表示   & 离散 ID（如 0, 1, ..., 79）                    & 语义嵌入向量（如 512 维）                 \\
        分类方式   & 学习特征到 ID 的映射                           & 计算特征与文本嵌入的相似度                \\
        新类别泛化 & 需要重新训练                                   & 输入新类别名称即可检测                    \\
        数学表达   & $f: \mathbb{R}^d \rightarrow \{0,1,...,N-1\} $ & $sim(f_{img}(x), f_{text}(t)) \in [0,1] $ \\
        \bottomrule
    \end{tabular}
    \caption{传统分类器与开放检测器的对比}
    \label{tab:classifier_compare}
\end{table}

开放检测器的核心思想是：\textbf{将目标检测从"特征到 ID 的映射"转化为"区域-文本匹配"问题}。给定图像区域特征 $v \in \mathbb{R}^d $ 和类别名称的文本嵌入 $t \in \mathbb{R}^d $，通过计算它们在统一语义空间中的相似度来判定类别：
\[
score(v, t) = \frac{v \cdot t}{\|v\| \|t\|} = \cos(v, t)
\]
这种设计的优势在于：只要文本编码器能够将新类别名称编码为语义向量，模型就能在零样本（Zero-shot）情况下识别该类别，无需重新训练。

\subsubsection{CLIP：视觉-语言对齐的基石}
这一范式转移的理论基础源于 OpenAI 的 CLIP（Contrastive Language-Image Pre-training）模型\cite{radford2021learning}。CLIP 通过在 4 亿图文对上进行对比学习，构建了一个统一的视觉-语言特征空间。

CLIP 的核心机制包括三个方面。\textbf{对比学习目标}：最大化匹配图文对的相似度，最小化不匹配对的相似度。给定一个 batch 包含 $N$ 个图文对 $\{(I_i, T_i)\}_{i=1}^N $，InfoNCE 损失为：
\[
\mathcal{L}_{CLIP} = -\frac{1}{N}\sum_{i=1}^{N}\log\frac{\exp(sim(I_i, T_i)/\tau)}{\sum_{j=1}^{N}\exp(sim(I_i, T_j)/\tau)}
\]
其中 $\tau$ 是温度参数，$sim(\cdot, \cdot)$ 是余弦相似度。\textbf{零样本迁移能力}：通过将类别名称转换为 "a photo of a \{class\}" 的文本提示，CLIP 能够在未见过的类别上进行分类。这一能力的关键在于：模型在预训练阶段学习到的是\textbf{物体的视觉-语义关联}，而非特定的类别 ID。\textbf{语义空间的连续性}：在 CLIP 的特征空间中，语义相近的概念（如 "dog" 和 "puppy"）在向量空间中距离较近，这为细粒度识别和泛化提供了基础。

然而，尽管 CLIP 在图像级分类上表现出色，但直接将其应用于目标检测面临挑战：CLIP 的预训练是在完整图像与文本之间进行的，缺乏对\textbf{局部区域}的精细理解；目标检测需要同时完成\textbf{定位}任务，而 CLIP 仅提供分类能力；区域特征与图像级特征的分布存在差异（Region-Image Distribution Gap）。

因此，开放目标检测的核心技术难题是：\textbf{如何将 CLIP 的图像级语义对齐能力迁移到区域级检测任务}。

\subsubsection{区域-文本对齐的技术路径}
为了解决上述问题，研究者提出了两条主要技术路径。\textbf{知识蒸馏路径（ViLD, RegionCLIP 等）}：利用预训练的 CLIP 作为教师模型，通过蒸馏将其知识迁移到检测器的区域嵌入中。然而，该方法受限于训练数据的类别覆盖，泛化能力有限，且蒸馏过程可能导致语义信息损失。\textbf{大规模接地预训练路径（GLIP, Grounding DINO 等）}：将目标检测重新定义为\textbf{短语接地（Phrase Grounding）}任务，即给定图像和文本描述，定位文本中提到的物体。该方法在大规模数据上进行预训练，构建区域-文本对 $\{(r_i, t_i)\} $，其中 $r_i$ 是图像区域，$t_i$ 是对应的文本描述（类别名或短语），通过对比学习使区域特征与文本特征在同一语义空间中对齐。

这种接地预训练的优势在于：直接在区域级进行训练，避免了图像级到区域级的迁移 gap；可以利用更丰富的文本信息（如属性、关系），而不仅仅是类别名；通过大规模数据（如数百万图文对）显著提升泛化能力。

\subsection{核心概念界定与研究范围}
在打破封闭世界假设的探索中，学术界从不同角度定义了"开放性"，形成了一个相互关联的研究谱系。根据对"开放性"理解的不同，主要包括以下研究方向：
\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{p{2.2cm}p{2.5cm}p{2.8cm}p{2.2cm}p{2.8cm}}
        \toprule
        研究方向 & 核心问题 & 对"开放"的定义 & 实用性 & 代表工作 \\
        \midrule
        开放词汇目标检测 (OVD) & 识别任意文本描述的类别 & 类别空间可通过语言无限扩展 & 成熟，零样本能力强 & GLIP, Grounding DINO, YOLO-World \\
        \midrule
        传统开放世界目标检测 (OWOD) & 主动发现未知并持续学习 & 能识别"不知道"并动态扩展知识 & 效果受限，召回率低，噪声大 & ORE, OW-DETR, PROB \\
        \midrule
        基于OVD的开放世界检测（OVD+OWOD） & 结合零样本与主动发现 & 利用OVD能力实现OWOD目标 & 新兴方向，性能显著提升 & OW-OVD, YOLO-UniOW \\
        \midrule
        开放集识别 (OSR) & 拒绝识别不可信样本 & 区分已知和未知，但不学习未知 & 成熟但应用受限 & OpenMax, ARPL \\
        \midrule
        零样本检测 (ZSD) & 基于属性推理未见类别 & 通过语义属性迁移到新类 & 依赖属性标注，泛化性有限 & DELO, SB \\
        \midrule
        少样本检测 (Few-shot) & 用少量样本学习新类 & 快速适应，但需要标注 & 实用但需标注成本 & Meta R-CNN, FSCE \\
        \midrule
        长尾检测 (Long-tail) & 处理类别分布不平衡 & 提升稀有类别性能 & 成熟，针对特定场景 & LVIS-based方法 \\
        \bottomrule
    \end{tabular}
    \caption{开放目标检测相关研究方向对比}
    \label{tab:research_direction}
\end{table}

\subsubsection{开放词汇目标检测 (OVD)}
给定图像 $I$ 和文本词汇表 $\mathcal{V} = \{t_1, t_2, ..., t_C\} $（$t_i$ 可以是类别名、短语或描述），开放词汇目标检测器 $\mathcal{D}_{OVD}$ 输出检测结果：
\[
\mathcal{D}_{OVD}(I, \mathcal{V}) = \{(b_i, c_i, s_i)\}_{i=1}^{N}
\]
其中 $b_i \in \mathbb{R}^4$ 是边界框坐标，$c_i \in \mathcal{V}$ 是匹配的文本类别，$s_i \in [0,1]$ 是置信度分数。

OVD 的核心特征包括：\textbf{动态词汇表}，即 $\mathcal{V}$ 在推理时可以任意指定，不受训练数据限制；\textbf{零样本泛化}，当 $\mathcal{V}$ 包含训练时未见过的类别（Novel Classes）时，模型仍能检测；\textbf{文本提示驱动}，用户通过输入自然语言（如 "a blue surfboard"）来指定检测目标。

OVD 的评估范式主要包括：\textbf{Zero-shot Transfer}，在 Base 类别上训练，在 Novel 类别上测试（如 COCO 的 48/17 划分，48 个 Base 类 + 17 个 Novel 类）；\textbf{Open Vocabulary Evaluation}，在大规模数据预训练后，直接在标准数据集（如 LVIS 1203 类）上零样本评估；\textbf{Referring Expression Comprehension (REC)}，给定描述性文本（如 "the person wearing a red hat"），定位对应目标。

\textbf{典型数据集：}
\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}lccc@{}}
        \toprule
        数据集      & 类别数     & 特点                             & 用途                \\
        \midrule
        COCO        & 80         & 常见物体，均衡分布               & Base/Novel 划分评估 \\
        LVIS        & 1203       & 长尾分布（rare/common/frequent） & 大词汇量零样本评估  \\
        ODinW       & 35个数据集 & 真实场景多样性                   & 跨域泛化评估        \\
        RefCOCO/+/g & -          & 包含属性和关系的描述             & 细粒度理解评估      \\
        \bottomrule
    \end{tabular}
    \caption{OVD 典型数据集}
    \label{tab:ovd_dataset}
\end{table}

\textbf{局限性分析：}

OVD 的本质是"\textbf{被动式开放}"，假设所有感兴趣的物体都能被提前命名和描述，模型依赖用户提供准确的类别名称或描述，无法主动发现用户未预期的新物体。

例如，如果用户不知道画面中出现了一只"狐猴"（Lemur），即使模型具备识别能力，也无法将其检测出来，因为用户没有在词汇表中包含该类别。

\subsubsection{传统开放世界目标检测 (OWOD)}
传统开放世界目标检测强调模型在动态环境中的\textbf{持续学习}能力。任务被划分为一系列增量学习子任务 $\mathcal{T} = \{T_1, T_2, ..., T_M\}$：在子任务 $T_i$ 训练时，模型学习新类别集合 $\mathcal{K}_{T_i}$；已知类别集合为 $\mathcal{K}_{known}^{(i)} = \mathcal{K}_{T_1} \cup ... \cup \mathcal{K}_{T_i}$；推理时，模型需要检测已知类别 $\mathcal{K}_{known}^{(i)}$，\textbf{主动识别未知物体}并标记为 "Unknown"，在后续任务中，部分 Unknown 类别被标注后加入 $\mathcal{K}_{T_{i+1}}$。

OWOD 的核心特征包括：\textbf{主动未知检测}，模型无需用户输入，自动标记训练集外的物体为 "Unknown"；\textbf{增量学习}，在学习新类别 $\mathcal{K}_{new}$ 时，保持对旧类别 $\mathcal{K}_{old}$ 的识别能力（避免灾难性遗忘）；\textbf{动态类别扩展}，类别集合随时间不断增长：$\mathcal{K}^{(1)} \subset \mathcal{K}^{(2)} \subset ... \subset \mathcal{K}^{(M)}$。

除了标准的 mAP，OWOD 引入专门的未知类别评估指标：\textbf{U-Recall}（未知类别的召回率，衡量模型发现未知物体的能力）、\textbf{U-mAP}（未知类别的平均精度，更严格地评估未知检测质量）、\textbf{Wilderness Impact (WI)}（未知物体对已知类别检测的干扰程度，理想值接近 0）、\textbf{Absolute Open-Set Error (A-OSE)}（综合评估未知检测和已知分类的平衡性）。

传统 OWOD 方法面临以下核心困难：\textbf{未知类别定义困难}，即什么是"Unknown"？模型如何在没有标签的情况下区分已知和未知？\textbf{伪标签噪声严重}，用模型自己的预测作为 Unknown 的监督信号，容易将背景、已知类别误判为未知，导致错误累积。\textbf{未知召回率低}，ORE 在 Task 1 仅获得 4.92 的 U-Recall，OW-DETR 也仅有约 7-9 的 U-Recall，远无法满足实际应用需求。\textbf{灾难性遗忘}，学习新类别时，旧类别的性能往往显著下降。

典型方法演进包括：\textbf{ORE (2021)} 首次提出 OWOD 任务设定，使用能量模型识别未知，但效果有限；\textbf{OW-DETR (2022)} 基于 DETR 架构，引入注意力驱动的伪标签生成，仍受制于噪声问题；\textbf{PROB (2023)} 利用概率建模区分已知和未知，性能略有提升但未根本解决问题。

\textbf{传统OWOD}由于缺乏对未知类别的有效建模方法,这些方法本质上是在"盲目猜测"什么是未知物体,导致性能瓶颈难以突破。

\subsubsection{基于OVD的开放世界检测:OVD+OWOD新范式}
近期研究发现,可以利用OVD检测器的强大零样本能力来解决OWOD任务,形成了一个新的研究范式。核心思想是:\textbf{将"未知"也视为一种可以被语言描述或建模的概念}。

下图展示了基于 OVD 实现开放世界检测的统一框架：
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../picture/p4.png}
    \caption{基于 OVD 的开放世界检测统一框架}
    \label{fig:p4}
\end{figure}

基于 OVD 的开放世界检测关键技术路线包括三个方面。\textbf{Foundation Models 辅助伪标签生成}：利用 SAM (Segment Anything Model) 生成候选未知区域，使用 GLIP、Grounding DINO 等 OVD 模型过滤已知类别，显著提升了未知物体伪标签的质量。\textbf{Wildcard Learning (通配符学习)}：YOLO-UniOW 为"未知"类别学习一个特殊的文本嵌入向量（wildcard），训练时将未标注区域与 wildcard 嵌入进行匹配，推理时同时输出已知类别和 unknown 类别的检测结果。\textbf{属性选择与不确定性融合}：OW-OVD 从标准 OVD 检测器出发，通过分析模型对不同语义属性的响应来识别未知，结合多个 OVD 检测器的预测不确定性，更准确地定位未知物体。

\textbf{性能提升:}

相比传统OWOD方法,基于OVD的统一框架取得了显著进步:
\begin{table}[htbp]
    \centering
    \begin{tabular}{@{}lccc@{}}
        \toprule
        方法类型             & U-Recall (Task 1) & mAP (已知类别) & 核心优势              \\
        \midrule
        ORE (传统)           & 4.9               & $\sim$50            & 首次提出任务,效果受限 \\
        OW-DETR (传统)       & 7-9               & $\sim$52            & DETR架构,注意力驱动   \\
        OW-OVD (基于OVD)     & \textbf{22+}      & $\sim$56            & 属性选择+不确定性融合 \\
        YOLO-UniOW (基于OVD) & \textbf{20+}      & $\sim$58            & 通配符学习+实时性     \\
        \bottomrule
    \end{tabular}
    \caption{不同OWOD方法性能对比}
    \label{tab:owod_performance}
\end{table}

基于OVD的方法在未知召回率上实现了\textbf{2-3倍的提升},同时保持了已知类别的检测精度。

\subsubsection{OVD与开放世界检测的对比与联系}
下表总结了三类方法的核心差异:
\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{p{2.2cm}p{3.5cm}p{3.5cm}p{3.5cm}}
        \toprule
        对比维度 & OVD & 传统OWOD & 基于OVD的开放世界检测 \\
        \midrule
        核心目标 & 识别用户指定的任意类别 & 主动发现未知物体并持续学习 & \textbf{统一}：零样本识别+主动发现 \\
        \midrule
        输入需求 & 需要文本提示（类别名/描述） & 无需文本输入 & 训练时无需，推理时可选 \\
        \midrule
        对未知的态度 & \textbf{被动识别}：仅识别词汇表中的类别 & \textbf{主动感知}：自动标记未训练类别 & \textbf{双重能力}：既能被动响应，又能主动发现 \\
        \midrule
        学习范式 & 零样本迁移（预训练 + 冻结） & 增量学习（持续更新） & 预训练OVD + 增量适配 \\
        \midrule
        评估重点 & 新类别的检测精度 & 未知发现 + 持续学习 + 防遗忘 & 两者兼顾 \\
        \midrule
        应用场景 & 用户知道要找什么（如搜索特定物品） & 模型需自主感知环境变化（如安防异常检测） & \textbf{通用场景}：同时支持指定检测和主动发现 \\
        \bottomrule
    \end{tabular}
    \caption{OVD、传统OWOD与基于OVD的开放世界检测对比}
    \label{tab:ovd_owod_compare}
\end{table}

三类方法的核心关系如下：OVD 提供了基础能力，即强大的零样本检测和视觉-语言理解；传统 OWOD 提出了目标愿景，即主动发现未知并持续学习，但技术实现不足；基于 OVD 的开放世界检测实现了能力融合，利用 OVD 的成熟技术来实现 OWOD 的目标，性能远超传统方法。

\subsection{总结：技术演进路线图}
为便于理解，我们将该领域的演进总结为以下4个阶段：
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../picture/p5.jpg}
    \caption{开放目标检测技术演进路线图}
    \label{fig:p5}
\end{figure}

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{p{2.2cm}p{2.2cm}p{2cm}p{3cm}p{2.8cm}p{2.5cm}}
        \toprule
        发展阶段 & 核心范式 & 类别表示 & 对未知的态度 & 代表模型 & 关键突破 \\
        \midrule
        \textbf{封闭世界} (~2020) & 特征到ID映射 & 离散数字ID & \textbf{排斥/忽略}：未知物体被误分类或当作背景 & Faster R-CNN, YOLOv5 & 精度与速度的极致优化 \\
        \midrule
        \textbf{开放词汇突破} (2021-2023) & 区域-文本匹配 & 文本嵌入向量 & \textbf{被动识别}：仅识别用户指定的类别名称 & GLIP, Grounding DINO, YOLO-World & CLIP + 大规模接地预训练 \\
        \midrule
        \textbf{传统开放世界} (2021-2023) & 主动发现 + 增量学习 & 物体性 + 动态扩展 & \textbf{主动感知}：自动标记未知但效果有限 & ORE, OW-DETR, PROB & 任务范式创新 \\
        \midrule
        \textbf{统一开放世界} (2024-) & \textbf{OVD + OWOD 融合} & 文本嵌入 + Wildcard & \textbf{双重能力}：零样本识别 + 主动发现 & OW-OVD, YOLO-UniOW & 基于OVD实现OWOD \\
        \bottomrule
    \end{tabular}
    \caption{开放目标检测技术演进阶段总结}
    \label{tab:tech_evolution}
\end{table}

本文聚焦于\textbf{开放词汇目标检测(OVD)}及其在\textbf{开放世界场景下的扩展应用(OVD+OWOD)}。首先，从\textbf{技术成熟度}来看，OVD 通过 CLIP 等大规模视觉-语言预训练取得了突破性进展，在零样本检测任务上已展现出强大的泛化能力和实用价值。其次，\textbf{传统 OWOD 的局限}在于，虽然传统 OWOD（如 ORE、OW-DETR）首次提出了主动发现未知物体的概念，但由于缺乏有效的未知物体建模方法，实际效果不佳——未知召回率普遍低于 10\%，且伪标签噪声严重。第三，\textbf{（OVD+OWOD）新范式的出现}：近期研究（OW-OVD、YOLO-UniOW 等）发现，可以基于成熟的 OVD 检测器，通过引入"未知"类别的特殊建模（如 Wildcard Learning、属性选择等），实现 OWOD 的目标，性能远超传统方法。最后，\textbf{统一框架的价值}在于，理想的开放感知系统应同时具备\textbf{被动响应}（用户指定类别）和\textbf{主动发现}（自动标记未知）的能力，这正是基于 OVD 实现统一开放世界检测的核心价值。

因此,本文将深入探讨OVD的核心技术,以及如何在OVD基础上实现开放世界能力，即OVD+OWOD,而对传统OWOD方法仅作简要介绍。

\section{OVD 核心技术范式——从深度融合到实时推理}
在目标检测领域，从“封闭世界”向“开放世界”转化的核心技术枢纽在于开放词汇目标检测（ OVD）。本部分将深入探讨 OVD 的两种主流技术范式：一种是以 Grounding DINO 为代表，追求极致语义对齐与定位精度的“高精度流派”；另一种是以 YOLO-World 为代表，致力于工业级端到端部署的“实时化流派”。这两者共同构成了当前开放感知领域的技术基石。

\subsection{高精度 OVD 框架：基于 Transformer 的多阶段深度对齐 (Grounding DINO)}
在开放词汇检测（OVD）任务中，核心挑战在于如何将离散的文本标签与连续的视觉空间表示进行对齐。Grounding DINO 提出了一种范式转变的思路：它不再将视觉和文本视为独立模态，而是基于“Grounded Pre-training”思想，摒弃了传统检测器仅在逻辑层进行类别映射的局限。该模型通过在 Transformer 的编码器、查询初始化及解码器全生命周期内引入强力的文本干预，实现了视觉与语言的深度耦合。

下图展示了 Grounding DINO 的整体架构及其三阶段对齐流：
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../picture/p1.png}
    \caption{Grounding DINO 整体架构与三阶段对齐流}
    \label{fig:p1}
\end{figure}
\textit{注：该架构清晰呈现了文本与图像特征从早期特征增强，到语言引导的查询选择，再到最终跨模态解码的闭环过程。}

\subsubsection{跨模态特征增强器}
传统开放集检测器通常采用"双塔架构"，仅在最后的分类头进行浅层的点积对齐，这导致视觉特征缺乏语义指导。Grounding DINO 引入了\textbf{跨模态增强器}，在特征提取的早期阶段实现视觉与语言的深度耦合。该模块由堆叠的 Transformer 层构成，利用 \textbf{Deformable Self-Attention} 高效处理多尺度视觉特征，并通过双向跨模态注意力实现信息流转：\textbf{视觉到文本的引导}，图像特征 $V$ 作为 Query，通过注意力机制感知文本中的关键描述，从而调整空间权值分布；\textbf{文本到视觉的注入}，文本特征 $T$ 注入到视觉空间，利用全局上下文消除语义歧义。
其数学表达可简述为：
\[
V', T' = \text{Bi-MultiHeadAttention}(V, T)
\]
这种\textbf{紧密融合}策略确保了视觉特征在进入预测头之前，就已经过语义过滤，极大增强了模型对长尾类别及复杂指代性表达的感知精度。

\subsubsection{语言引导的查询选择}
为了在无限的语义空间中精确定位，Grounding DINO 改进了标准 DINO 的查询机制，利用语言先验来初始化对象查询。\textbf{对比驱动的初始化}：模型不再使用随机的 Learnable Queries，而是利用 Enhancer 输出的图像特征 $f_v$ 与文本特征 $f_t$ 计算相似度分数。\textbf{筛选逻辑}：根据相似度分数筛选出 Top-K 个图像高响应区域的索引：
\[
\text{Indices} = \text{Top-K}(\text{Softmax}(f_v \cdot f_t^\top))
\]
这些索引被用于初始化 \textbf{Dynamic Anchor Boxes} 的位置坐标，而内容查询（Content Queries）则保持可学习状态。这种设计使得解码器从一开始就具备了“语义先验”，能够直接跳过无关区域，聚焦于与 Prompt 高度相关的候选目标，从而在零样本检测任务中表现出极强的鲁棒性。

例如，假设 Prompt 是“穿着红色雨衣的小狗”，在这一阶段，模型会根据“红色”、“雨衣”、“狗”的联合语义分布，在复杂的背景中直接定位到那几个高相关的像素块。查询向量（Queries）从诞生的那一刻起，就携带了目标的几何位置先验。

\subsubsection{跨模态解码器与子句级去噪}
在解码阶段，Grounding DINO 引入了定制化的 \textbf{Text Cross-Attention} 层，并配合子句级处理机制，进一步细化对齐粒度。\textbf{子句级掩码}：针对长文本描述，模型构建了特殊的 $\text{Attention Mask}$。该机制强制每个视觉区域仅与对应的名词短语交互，屏蔽句中无关词汇，从而显著降低了 OVD 场景中的假阳性概率。例如，在处理指令 "A man holding a blue umbrella" 时，模型通过遮罩确保 "man"、"holding" 和 "blue umbrella" 分别对应不同的跨模态权重。这种设计防止了语义干扰，不会因为出现了"blue"就让模型去寻找蓝色的衣服，而是严格将其限制在"umbrella"的属性上，确保了细粒度属性的精确归属。\textbf{循环交互解码}：解码器层通过层级迭代，不断利用文本特征作为辅助信息来更新边界框坐标。此时，文本特征不仅是分类的依据，更作为空间偏移的偏置项直接参与了边界框（BBox）的回归计算，引导模型对遮挡或边缘模糊的目标进行精确重建。通过多层循环迭代，模型能对遮挡或尺度极小的目标（如远景中的车辆）进行像素级的坐标微调。
Grounding DINO 专门设计了 $\text{Text Cross-Attention}$ 层，用于将文本信息注入到 Query 中，以实现更好的模态对齐。

\subsubsection{分类与定位的对齐损失}
除此以外，Grounding DINO 针对开放集特性重新设计了分类损失。它摒弃了传统的固定类别 Cross-Entropy，而是基于\textbf{点积相似度}计算预测结果，并采用 \textbf{Focal Loss} 进行优化：
\[
\mathcal{L}_{cls} = \text{FocalLoss}(\text{Softmax}(f_{query} \cdot f_{text}^\top), y_{gt})
\]
该机制计算预测出的 BBox 视觉嵌入 $f_{query}$ 与文本标签 $f_{text}$ 的相似度，强制正样本对在向量空间中产生高响应，而与无关文本保持低响应。这种设计有效地解决了正负样本极度不平衡的问题，使得模型在 Zero-shot（零样本）迁移时能根据语义相似度进行准确判别。

此外，在定位分支，Grounding DINO 依然保留了标准的 \textbf{L1 Loss} 和 \textbf{GIOU Loss} 来确保边界框回归的几何精度。

\subsection{实时化 OVD 框架：重参数化带来的感知革命 (YOLO-World)}
虽然Grounding DINO 等基于 Transformer 为核心的框架在精度上屡创新高，但在边缘计算和实时监控场景下，其高昂的计算成本（FLOPs）和推理延迟成为了瓶颈。YOLO-World 的出现填补了这一空白，它实现了“开集检测”与“实时推理”的完美统一。该模型在 V100 上能以 52.0 FPS 的速度实现高精度的零样本检测，证明了轻量级检测器通过架构创新，同样可以驾驭复杂的开放词汇场景。

下图展示了 YOLO-World 的整体架构，清晰呈现了其如何将文本编码器、YOLO 骨干网以及特征融合模块有机结合：
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../picture/p2.png}
    \caption{YOLO-World 整体架构}
    \label{fig:p2}
\end{figure}
\textit{注：如图所示，YOLO-World 包含图像编码器、文本编码器以及核心的 RepVL-PAN 模块，最后通过对比头输出目标位置与类别嵌入。}

\subsubsection{RepVL-PAN与视觉-语义交互}
YOLO-World 并没有简单地将文本特征拼接到 YOLO 的 Head 上，而是设计了一种全新的\textbf{重参数化视觉-语言路径聚合网络（RepVL-PAN）}。在训练阶段，模型引入了文本引导的 CSPLayer，通过卷积算子与文本向量的交互，动态地调整视觉特征的权重。不同于 Transformer 的全对齐，RepVL-PAN 侧重于利用文本信息作为“滤波器”，对视觉特征进行通道级的重构，从而在保留 YOLO 实时性的同时注入了语义敏感度。此外，为了增强全局语义理解，模型在颈部网络中嵌入了图像池化注意力，使得 YOLO 这种局部感知见长的模型也能获得大尺度语义视野。

\subsubsection{“先提示，后检测”：推理效率的质变}
与 Grounding DINO 的实时交互不同，YOLO-World 采用了“先提示后检测（Prompt-then-detect）”的范式。在推理时，文本编码器可以被移除，文本嵌入被重新参数化为网络权重。

在实际应用中，用户通常会预设一组感兴趣的类别（Offline Vocabulary）。YOLO-World 允许将这些类别的文本嵌入通过重参数化技术提前融合进检测头的卷积权重中。这意味着：在推理过程中，模型无需运行文本编码器，也无需进行在线的跨模态注意力计算。从计算链路上看，它退化为了一个纯视觉的 YOLO 模型，从而在 V100 等设备上实现了超过 50 FPS 的惊人速度。这种"离线编码、在线匹配"的逻辑，彻底解决了 OVD 落地难的痛点。具体而言，用户输入的类别名称通过 Text Encoder 转化为文本嵌入矩阵 $W_{text}$，模型将该矩阵预先计算，并将其数值直接作为检测头中 $1 \times 1$ 卷积层的卷积核参数。

\subsubsection{区域-文本对比学习策略}
为了在大规模无标注数据中学习泛化性，YOLO-World 引入了区域-文本对比损失。通过将提取到的候选框特征与大规模图文对（如 CC3M）中的文本描述进行距离测算，模型学到了如何在一个统一的、高维的语义空间中对物体进行归类。这使得它即便在没有经过精细标注的类别（如某些特定品牌的商品）上，也能凭借大规模预训练带来的常识进行准确预测。

\subsection{对比分析}
总结来看，Grounding DINO 代表了开放目标检测的"上限"，它通过复杂的 Transformer 交互实现了对复杂指令的精准解析，是离线分析和高质量标注任务的首选。而 YOLO-World 则代表了"广度"，它利用重参数化技术将开放能力平民化，让实时嵌入式设备具备了识别万物的可能。

从架构设计角度看，\textbf{架构的"高保真"视觉提取}：Grounding DINO 这种深度融合模型虽然强大，但视觉特征被语言高度"污染"了。在 OWD 任务中，我们需要发现那些"没有名字（Unknown）"的物体。YOLO-World 的重参数化设计使得视觉骨干网络保留了更纯粹的物体显著性（Objectness）感知能力，更利于通过"通配符（Wildcard）"等技术捕捉未知目标。从学习效率角度看，\textbf{增量学习的极低成本}：OWD 需要模型能够不断学习新类别。基于重参数化的架构，学习新类别只需更新离线词汇表向量，而无需对整个庞大的 Transformer 网络进行微调。这为\textbf{高效增量学习}提供了天然的基础。从应用部署角度看，\textbf{计算资源的可扩展性}：开放世界任务通常涉及处理海量的无标注数据和动态视频流，高吞吐量（High Throughput）是基本要求。YOLO-World 的效率优势使其成为构建复杂感知系统的唯一可行基座。

这种从“重架构深度融合”向“轻量化重参数化”的演进，标志着 OVD 领域已经完成了从实验室方案向工业化可行方案的初步转型。而在接下来的第三部分中，我们将讨论如何在此基础上，进一步赋予模型“发现未知”的能力，即迈向真正的开放世界（OWD）。

\section{迈向开放与统一：OVD 向 OWD 的进阶与探索（基于 YOLO-World 提出的 OVD 框架）}
本部分介绍开放目标检测领域最具前沿性的挑战：如何将 OVD 的零样本泛化能力，与 OWD 的未知发现、持续学习能力进行统一。传统方法往往只专注于其中一个任务，而现实应用场景往往需要同时具备两种能力。本节将深入探讨两个代表性的统一框架：OW-OVD 和 YOLO-UniOW，它们分别从不同角度实现了 OVD 与 OWOD 的统一。

\subsection{OVD 与 OWD 的统一任务探索：OW-OVD}
\textit{论文：\textbf{OW-OVD: Unified Open World and Open Vocabulary Object Detection}\cite{owovd2025}}

OW-OVD\cite{owovd2025} 是首个明确提出要统一解决 OVD 和 OWOD 两个任务的检测器。其核心思想是在保持 OVD 检测器零样本泛化能力的同时，赋予其主动检测未知物体并通过增量学习持续优化的能力。与现有方法（如 FOMO）不同，OW-OVD 的关键创新在于\textbf{不改变 OVD 的标准推理过程}，从而确保其零样本能力不受影响。

\subsubsection{核心挑战与技术路线}
OW-OVD 面临的核心挑战是如何在不修改 OVD 推理流程的前提下，实现对未知物体的检测。传统方法如 FOMO 通过线性缩放属性相似度来预测未知，但这会改变 OVD 的标准推理过程，从而损害其零样本能力。OW-OVD 采用了一种更加巧妙的设计：\textbf{在保持 OVD 推理流程不变的基础上，通过属性选择和不确定性融合来识别未知物体}。

\subsubsection{视觉相似度属性选择（VSAS）方法}
OW-OVD 提出了\textbf{视觉相似度属性选择（Visual Similarity Attribute Selection, VSAS）}方法，用于识别在标注区域和未标注区域中都普遍存在的属性。该方法的核心思想是：未知物体与已知物体在某些通用属性上应该具有相似性，这些属性可以作为识别未知的线索。

VSAS 方法的具体流程如下：首先，利用目标检测中的标准匹配方法，将视觉嵌入分为正样本（标注区域）和负样本（背景或未标注区域）。然后，计算所有属性与视觉嵌入之间的相似度，并聚合这些相似度。在属性选择阶段，通过比较正样本和负样本的属性相似度分布差异，评估标注区域与未标注区域之间的差异。基于这些差异，选择在标注区域和未标注区域中都常见的属性。此外，为了防止选择的属性过于相似，OW-OVD 引入了相似度约束，确保所选属性的多样性。

这种设计的优势在于：通过选择通用属性，模型能够捕捉到未知物体与已知物体之间的共性，从而更好地识别未知物体，同时避免了过度依赖特定已知类别的特征。

\subsubsection{混合属性-不确定性融合（HAUF）方法}
为了预测未知物体，OW-OVD 提出了\textbf{混合属性-不确定性融合（Hybrid Attribute-Uncertainty Fusion, HAUF）}方法。该方法结合已知类别的不确定性和加权属性相似度，来估计给定视觉区域被分类为未知的可能性。

HAUF 方法的核心在于：\textbf{已知类别的不确定性}反映了模型对当前区域属于已知类别的置信度，而\textbf{加权属性相似度}则反映了该区域与通用属性的匹配程度。当已知类别的不确定性较高（即模型不确定该区域属于哪个已知类别），且属性相似度也较高时，该区域更可能属于未知类别。

这种融合策略的优势在于：它不需要修改 OVD 的标准推理过程，只是在推理结果的基础上进行额外的未知判断。这意味着 OVD 的零样本能力得以完整保留，同时模型又具备了主动发现未知物体的能力。

\subsubsection{性能评估与实验验证}
OW-OVD 在 OWOD 基准任务 M-OWODB 和 S-OWODB 上进行了验证。实验结果表明，OW-OVD 在已知类别和未知类别上都显著优于现有的最先进模型。具体而言，在 S-OWODB Task 1 上，OW-OVD 在未知物体召回率（U-Recall）上实现了 +15.3 的提升，在平均精度（mAP）上实现了 +4.3 的提升。更重要的是，在使用更严格的评估指标（U-mAP）时，OW-OVD 相比最先进模型取得了更大的性能优势（+15.5 U-mAP）。

这些结果证明了 OW-OVD 方法的有效性：它不仅成功统一了 OVD 和 OWOD 两个任务，还在保持 OVD 零样本能力的同时，显著提升了未知物体检测的性能。

\subsection{高效的通用开放世界检测范式：YOLO-UniOW}
\textit{论文：\textbf{YOLO-UniOW: Efficient Universal Open-World Object Detection}\cite{yolouniow2024}}

YOLO-UniOW\cite{yolouniow2024} 是在统一 OVD/OWOD 任务上追求\textbf{效率}和\textbf{通用性}的最新尝试。与 OW-OVD 不同，YOLO-UniOW 基于 YOLO-World 的高效架构，提出了一个更简洁、更高效的解决方案，特别适合实时应用场景。

\subsubsection{设计理念：基于 YOLO-World 的统一框架}
YOLO-UniOW 的核心设计理念是：\textbf{在 YOLO-World 的高效架构基础上，通过引入"通配符（Wildcard）"学习机制，实现对未知物体的检测}。这种设计的优势在于，它充分利用了 YOLO-World 的重参数化优势，使得未知检测和已知检测可以在同一个高效框架内完成。

与 OW-OVD 的属性选择方法不同，YOLO-UniOW 采用了一种更加直接的方法：\textbf{为"未知"类别学习一个特殊的文本嵌入向量（wildcard embedding）}。这个 wildcard 嵌入在训练时与未标注区域进行匹配，在推理时与已知类别的文本嵌入一起参与检测，从而实现了已知和未知的统一检测。

\subsubsection{通配符学习机制}
YOLO-UniOW 的\textbf{通配符学习（Wildcard Learning）}机制是其核心创新。在训练阶段，模型将未标注区域（即不属于任何已知类别的区域）与 wildcard 嵌入进行匹配。这种匹配过程使得模型学习到如何识别那些不属于已知类别但具有物体性的区域。

训练时，YOLO-UniOW 的损失函数包括两部分：已知类别的标准检测损失和未知类别的 wildcard 匹配损失。通过联合优化这两个损失，模型同时学习已知类别的检测和未知物体的识别。推理时，模型同时输出已知类别和 unknown 类别的检测结果，实现了真正的统一检测。

\subsubsection{高效增量学习能力}
YOLO-UniOW 的一个重要优势是其\textbf{高效的增量学习能力}。由于基于 YOLO-World 的重参数化架构，学习新类别只需更新离线词汇表向量，而无需对整个网络进行微调。这种设计使得模型能够以极低的成本适应新类别，非常适合实际应用中的持续学习场景。

当新的未知类别被标注后，YOLO-UniOW 可以将其文本嵌入直接添加到词汇表中，通过简单的重参数化即可实现对新类别的检测，而无需重新训练整个模型。这种增量学习机制大大降低了模型更新的成本，使得 YOLO-UniOW 在实际部署中具有显著优势。

\subsubsection{实时性能与通用性}
YOLO-UniOW 继承了 YOLO-World 的高效特性，在保持实时推理速度的同时，实现了对已知和未知物体的统一检测。这使得它特别适合需要实时响应的应用场景，如自动驾驶、智能监控等。

此外，YOLO-UniOW 的通用性体现在：它既可以作为标准的 OVD 检测器使用（当用户提供类别列表时），也可以作为 OWOD 检测器使用（当需要主动发现未知物体时），还可以同时支持两种模式。这种灵活性使得 YOLO-UniOW 能够适应多样化的应用需求。

\subsection{两种统一范式的对比与总结}
OW-OVD 和 YOLO-UniOW 代表了统一 OVD 和 OWOD 任务的两种不同技术路线。OW-OVD 采用属性选择和不确定性融合的方法，在不改变 OVD 推理过程的前提下实现未知检测，更适合需要保持 OVD 标准推理流程的场景。YOLO-UniOW 采用通配符学习机制，在高效架构基础上实现统一检测，更适合需要实时性能和高效增量学习的场景。

两种方法都成功证明了：\textbf{基于成熟的 OVD 检测器，通过适当的技术创新，可以实现 OVD 和 OWOD 的统一}。这不仅解决了传统 OWOD 方法性能不足的问题，还为构建真正通用的开放感知系统提供了可行的技术路径。未来，随着更多统一框架的出现，开放目标检测领域将朝着更加通用、高效的方向发展。

\bibliographystyle{plain}
\begin{thebibliography}{99}
    \bibitem{ren2017faster}
    Ren et al., "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", TPAMI 2017
    \bibitem{gupta2019lvis}
    Gupta et al., "LVIS: A Dataset for Large Vocabulary Instance Segmentation", CVPR 2019
    \bibitem{radford2021learning}
    Radford et al., "Learning Transferable Visual Models from Natural Language Supervision", ICML 2021
    \bibitem{owovd2025}
    Xing Xi, Yangyang Huang, Ronghua Luo, Yu Qiu, "OW-OVD: Unified Open World and Open Vocabulary Object Detection", CVPR 2025
    \bibitem{yolouniow2024}
    "YOLO-UniOW: Efficient Universal Open-World Object Detection", 2024
\end{thebibliography}
\end{document}
